package kmon

import (
	"context"
	"fmt"

	"slices"
	"strconv"
	"time"

	"github.com/google/uuid"
	"github.com/phuslu/log"
	"github.com/pliu/kmon/pkg/clients"
	"github.com/pliu/kmon/pkg/config"
	"github.com/pliu/kmon/pkg/utils"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/twmb/franz-go/pkg/kadm"
	"github.com/twmb/franz-go/pkg/kgo"
)

type partitionMetrics struct {
	p2b *utils.Stats
	e2e *utils.Stats
}

type Monitor struct {
	config         *config.KMonConfig
	producerClient clients.KgoClient
	consumerClient clients.KgoClient
	adminClient    clients.KadmClient
	instanceUUID   string
	partitions     []int32
	partitionStats map[int32]*partitionMetrics
}

// TODO: Move defaults into config
const (
	statsWindow     = 5 * time.Minute
	defaultSampleMs = 1000
	quantileP50     = 50
	quantileP95     = 95
	quantileP99     = 99
)

func NewMonitorWithClients(cfg *config.KMonConfig, producerClient clients.KgoClient, consumerClient clients.KgoClient, adminClient clients.KadmClient, instanceUUID string) *Monitor {
	return &Monitor{
		config:         cfg,
		producerClient: producerClient,
		consumerClient: consumerClient,
		adminClient:    adminClient,
		instanceUUID:   instanceUUID,
		partitionStats: make(map[int32]*partitionMetrics),
	}
}

func NewMonitorFromConfig(cfg *config.KMonConfig) (*Monitor, error) {
	if cfg == nil || cfg.ProducerKafkaConfig == nil {
		return nil, fmt.Errorf("producer kafka config is required")
	}

	producerOpts := []kgo.Opt{
		kgo.SeedBrokers(cfg.ProducerKafkaConfig.SeedBrokers...),
		kgo.RecordPartitioner(kgo.ManualPartitioner()),
	}
	if cfg.ConsumerKafkaConfig == nil {
		producerOpts = append(producerOpts, kgo.ConsumeTopics(cfg.ProducerMonitoringTopic))
	}

	producerClient, err := kgo.NewClient(producerOpts...)
	if err != nil {
		return nil, err
	}

	consumerClient := producerClient
	if cfg.ConsumerKafkaConfig != nil {
		consumerOpts := []kgo.Opt{
			kgo.SeedBrokers(cfg.ConsumerKafkaConfig.SeedBrokers...),
			kgo.ConsumeTopics(cfg.ConsumerMonitoringTopic),
		}

		consumerClient, err = kgo.NewClient(consumerOpts...)
		if err != nil {
			producerClient.Close()
			return nil, err
		}
	}

	adminClient := kadm.NewClient(producerClient)

	// Generate a unique UUID for this Monitor instance
	instanceUUID := uuid.NewString()

	return NewMonitorWithClients(cfg, &clients.KgoClientWrapper{Client: producerClient}, &clients.KgoClientWrapper{Client: consumerClient}, adminClient, instanceUUID), nil
}

func (m *Monitor) Start(ctx context.Context) {
	defer m.producerClient.Close()
	if m.consumerClient != m.producerClient {
		defer m.consumerClient.Close()
	}

	if err := m.initialisePartitions(ctx); err != nil {
		log.Error().Err(err).Msg("failed to initialise partitions")
		return
	}

	go m.consumeLoop(ctx)

	interval := m.sampleInterval()
	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			go m.publishProbeBatch(ctx)
		}
	}
}

func (m *Monitor) publishProbeBatch(ctx context.Context) {
	for _, partition := range m.partitions {
		m.publishProbe(ctx, partition)
	}
}

func (m *Monitor) publishProbe(ctx context.Context, partition int32) {
	sentAt := time.Now()
	topic := m.config.ProducerMonitoringTopic
	record := &kgo.Record{
		Topic:     topic,
		Partition: partition,
		Key:       []byte(m.instanceUUID),
		Value:     []byte(fmt.Sprintf("%d", sentAt.UnixNano())),
	}

	partitionLabel := m.partitionLabel(partition)

	m.producerClient.Produce(ctx, record, func(r *kgo.Record, err error) {
		if err != nil {
			ProduceFailureCount.WithLabelValues(partitionLabel).Inc()
			return
		}

		ackLatencyMs := float64(time.Since(sentAt).Milliseconds())
		m.getPartitionMetrics(partition).recordP2B(partitionLabel, ackLatencyMs)
	})
}

func (m *Monitor) consumeLoop(ctx context.Context) {
	for {
		if ctx.Err() != nil {
			return
		}

		fetches := m.consumerClient.PollFetches(ctx)
		if fetches.IsClientClosed() {
			return
		}

		fetches.EachError(func(topic string, partition int32, err error) {
			log.Error().Err(err).Msgf("kafka fetch error on %s[%d]", topic, partition)
		})

		fetches.EachRecord(func(record *kgo.Record) {
			go m.handleConsumedRecord(record)
		})
	}
}

func (m *Monitor) handleConsumedRecord(record *kgo.Record) {
	// Only process messages that were generated by this instance
	if string(record.Key) != m.instanceUUID {
		return
	}

	// If the message was produced by this instance, continue processing
	timestamp, err := strconv.ParseInt(string(record.Value), 10, 64)
	if err != nil {
		// If we can't parse the timestamp, ignore it
		return
	}
	sentAt := time.Unix(0, timestamp)

	consumeTime := time.Now()
	partition := record.Partition
	partitionLabel := m.partitionLabel(partition)

	e2eLatencyMs := float64(consumeTime.Sub(sentAt).Milliseconds())
	m.getPartitionMetrics(partition).recordE2E(partitionLabel, e2eLatencyMs)
}

func (m *Monitor) initialisePartitions(ctx context.Context) error {
	topic := m.config.ProducerMonitoringTopic
	log.Info().Msgf("Initialising partitions for topic %s", topic)
	metadata, err := m.adminClient.ListTopics(ctx, topic)
	if err != nil {
		log.Error().Err(err).Msg("failed to list topics")
		return err
	}

	details, ok := metadata[topic]
	if !ok {
		log.Error().Msgf("topic %s not found in metadata", topic)
		return fmt.Errorf("topic %s not found", topic)
	}

	log.Info().Msgf("Found %d partitions for topic %s", len(details.Partitions), topic)

	partitions := make([]int32, 0, len(details.Partitions))
	for partition := range details.Partitions {
		partitions = append(partitions, partition)
	}
	slices.Sort(partitions)

	m.partitions = partitions
	for _, partition := range partitions {
		m.partitionStats[partition] = newPartitionMetrics(statsWindow)
	}

	MonitoringTopicPartitionCount.Set(float64(len(partitions)))
	return nil
}

func (m *Monitor) getPartitionMetrics(partition int32) *partitionMetrics {
	return m.partitionStats[partition]
}

func (m *Monitor) partitionLabel(partition int32) string {
	return fmt.Sprintf("%d", partition)
}

// TODO: Move to config
func (m *Monitor) sampleInterval() time.Duration {
	intervalMs := m.config.SampleFrequencyMs
	if intervalMs <= 0 {
		intervalMs = defaultSampleMs
	}
	return time.Duration(intervalMs) * time.Millisecond
}

func newPartitionMetrics(window time.Duration) *partitionMetrics {
	return &partitionMetrics{
		p2b: utils.NewStats(window),
		e2e: utils.NewStats(window),
	}
}

func (pm *partitionMetrics) recordP2B(partitionLabel string, latencyMs float64) {
	pm.p2b.Add(int64(latencyMs))
	P2BMessageLatencyHistogram.WithLabelValues(partitionLabel).Observe(latencyMs)
	pm.updateQuantiles(pm.p2b, P2BMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) recordE2E(partitionLabel string, latencyMs float64) {
	pm.e2e.Add(int64(latencyMs))
	E2EMessageLatencyHistogram.WithLabelValues(partitionLabel).Observe(latencyMs)
	pm.updateQuantiles(pm.e2e, E2EMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) updateQuantiles(stats *utils.Stats, gauge *prometheus.GaugeVec, partitionLabel string) {
	res, ok := stats.Percentile([]float64{quantileP50, quantileP95, quantileP99})
	if !ok || len(res) < 3 {
		return
	}
	gauge.WithLabelValues(partitionLabel, "p50").Set(float64(res[0]))
	gauge.WithLabelValues(partitionLabel, "p95").Set(float64(res[1]))
	gauge.WithLabelValues(partitionLabel, "p99").Set(float64(res[2]))
}
