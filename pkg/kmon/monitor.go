package kmon

import (
	"context"
	"fmt"

	"strconv"
	"time"

	"github.com/google/uuid"
	"github.com/phuslu/log"
	"github.com/pliu/kmon/pkg/clients"
	"github.com/pliu/kmon/pkg/config"
	"github.com/pliu/kmon/pkg/utils"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/twmb/franz-go/pkg/kgo"
)

type partitionMetrics struct {
	p2b *utils.Stats
	b2c *utils.Stats
	e2e *utils.Stats
}

type Monitor struct {
	producerClient  clients.KgoClient
	producerTopic   string
	consumerClient  clients.KgoClient
	instanceUUID    string
	partitions      []int32
	partitionStats  map[int32]*partitionMetrics
	sampleFrequency time.Duration
}

func NewMonitorWithClients(producerClient clients.KgoClient, producerTopic string, consumerClient clients.KgoClient, instanceUUID string, partitions []int32, sampleFrequency time.Duration, statsWindow time.Duration) *Monitor {
	m := &Monitor{
		producerClient:  producerClient,
		producerTopic:   producerTopic,
		consumerClient:  consumerClient,
		instanceUUID:    instanceUUID,
		partitions:      partitions,
		sampleFrequency: sampleFrequency,
	}
	m.partitionStats = make(map[int32]*partitionMetrics)
	for _, p := range m.partitions {
		m.partitionStats[p] = newPartitionMetrics(statsWindow)
	}
	return m
}

// TODO: Cross-cluster measurements should ignore partitions on e2e and not measure b2c
func NewMonitorFromConfig(cfg *config.KMonConfig, partitions []int32) (*Monitor, error) {
	var producerClient *kgo.Client
	var consumerClient *kgo.Client
	var err error

	if cfg.ConsumerKafkaConfig == nil {
		producerClient, err = clients.GetFranzGoClient(cfg.ProducerKafkaConfig, cfg.ProducerMonitoringTopic)
		if err != nil {
			return nil, err
		}
		consumerClient = producerClient
	} else {
		producerClient, err = clients.GetFranzGoClient(cfg.ProducerKafkaConfig)
		if err != nil {
			return nil, err
		}
		consumerClient, err = clients.GetFranzGoClient(cfg.ConsumerKafkaConfig, cfg.ConsumerMonitoringTopic)
		if err != nil {
			producerClient.Close()
			return nil, err
		}
	}

	instanceUUID := uuid.NewString()

	return NewMonitorWithClients(producerClient, cfg.ProducerMonitoringTopic, consumerClient, instanceUUID, partitions, time.Duration(cfg.GetSampleFrequencyMs())*time.Millisecond, time.Duration(cfg.GetStatsWindowSeconds())*time.Second), nil
}

func (m *Monitor) Start(ctx context.Context) {
	defer m.producerClient.Close()
	if m.consumerClient != m.producerClient {
		defer m.consumerClient.Close()
	}

	m.Warmup(ctx)

	go m.consumeLoop(ctx)

	ticker := time.NewTicker(m.sampleFrequency)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			go m.publishProbeBatch(ctx)
		}
	}
}

func (m *Monitor) Warmup(ctx context.Context) {
	m.publishProbeBatch(ctx)
	time.Sleep(3 * time.Second)
}

func (m *Monitor) publishProbeBatch(ctx context.Context) {
	for _, partition := range m.partitions {
		m.publishProbe(ctx, partition)
	}
}

func (m *Monitor) publishProbe(ctx context.Context, partition int32) {
	sentAt := time.Now()
	record := &kgo.Record{
		Topic:     m.producerTopic,
		Partition: partition,
		Key:       []byte(m.instanceUUID),
		Value:     fmt.Appendf(nil, "%d", sentAt.UnixNano()),
	}

	partitionLabel := m.partitionLabel(partition)

	m.producerClient.Produce(ctx, record, func(r *kgo.Record, err error) {
		if err != nil {
			ProduceMessageFailureCount.WithLabelValues(partitionLabel).Inc()
			return
		}

		ackLatencyMs := float64(time.Since(sentAt).Milliseconds())
		m.getPartitionMetrics(partition).recordP2B(partitionLabel, ackLatencyMs)
		ProduceMessageCount.WithLabelValues(partitionLabel).Inc()
	})
}

func (m *Monitor) consumeLoop(ctx context.Context) {
	for {
		fetches := m.consumerClient.PollFetches(ctx)

		select {
		case <-ctx.Done():
			return
		default:
			if fetches.IsClientClosed() {
				return
			}

			fetches.EachError(func(topic string, partition int32, err error) {
				log.Error().Err(err).Msgf("kafka fetch error on %s[%d]", topic, partition)
			})

			fetches.EachRecord(func(record *kgo.Record) {
				go m.handleConsumedRecord(record)
			})
		}
	}
}

func (m *Monitor) handleConsumedRecord(record *kgo.Record) {
	// Only process messages that were generated by this instance
	if string(record.Key) != m.instanceUUID {
		return
	}

	timestamp, err := strconv.ParseInt(string(record.Value), 10, 64)
	if err != nil {
		// TODO: Log, metric?
		return
	}
	sentAt := time.Unix(0, timestamp)

	consumeTime := time.Now()
	partition := record.Partition
	partitionLabel := m.partitionLabel(partition)

	e2eLatencyMs := float64(consumeTime.Sub(sentAt).Milliseconds())
	b2cLatencyMs := float64(consumeTime.Sub(record.Timestamp).Milliseconds())
	partitionMetrics := m.getPartitionMetrics(partition)
	partitionMetrics.recordE2E(partitionLabel, e2eLatencyMs)
	partitionMetrics.recordB2C(partitionLabel, b2cLatencyMs)
	ConsumeMessageCount.WithLabelValues(partitionLabel).Inc()
}

func (m *Monitor) getPartitionMetrics(partition int32) *partitionMetrics {
	return m.partitionStats[partition]
}

func (m *Monitor) partitionLabel(partition int32) string {
	return fmt.Sprintf("%d", partition)
}

func newPartitionMetrics(window time.Duration) *partitionMetrics {
	return &partitionMetrics{
		p2b: utils.NewStats(window),
		b2c: utils.NewStats(window),
		e2e: utils.NewStats(window),
	}
}

func (pm *partitionMetrics) recordP2B(partitionLabel string, latencyMs float64) {
	pm.p2b.Add(int64(latencyMs))
	pm.updateQuantiles(pm.p2b, P2BMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) recordB2C(partitionLabel string, latencyMs float64) {
	pm.b2c.Add(int64(latencyMs))
	pm.updateQuantiles(pm.b2c, B2CMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) recordE2E(partitionLabel string, latencyMs float64) {
	pm.e2e.Add(int64(latencyMs))
	pm.updateQuantiles(pm.e2e, E2EMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) updateQuantiles(stats *utils.Stats, gauge *prometheus.GaugeVec, partitionLabel string) {
	res, ok := stats.Percentile([]float64{50, 95, 99})
	if !ok {
		return
	}
	gauge.WithLabelValues(partitionLabel, "p50").Set(float64(res[0]))
	gauge.WithLabelValues(partitionLabel, "p95").Set(float64(res[1]))
	gauge.WithLabelValues(partitionLabel, "p99").Set(float64(res[2]))
}
