package kmon

import (
	"context"
	"fmt"

	"strconv"
	"time"

	"github.com/google/uuid"
	"github.com/phuslu/log"
	"github.com/pliu/kmon/pkg/clients"
	"github.com/pliu/kmon/pkg/config"
	"github.com/pliu/kmon/pkg/utils"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/twmb/franz-go/pkg/kgo"
)

type partitionMetrics struct {
	p2b *utils.Stats
	e2e *utils.Stats
}

type Monitor struct {
	producerClient  clients.KgoClient
	producerTopic   string
	consumerClient  clients.KgoClient
	instanceUUID    string
	partitions      []int32
	partitionStats  map[int32]*partitionMetrics
	sampleFrequency time.Duration
}

func NewMonitorWithClients(producerClient clients.KgoClient, producerTopic string, consumerClient clients.KgoClient, instanceUUID string, partitions []int32, sampleFrequency time.Duration, statsWindow time.Duration) *Monitor {
	m := &Monitor{
		producerClient:  producerClient,
		producerTopic:   producerTopic,
		consumerClient:  consumerClient,
		instanceUUID:    instanceUUID,
		partitions:      partitions,
		sampleFrequency: sampleFrequency,
	}
	m.partitionStats = make(map[int32]*partitionMetrics)
	for _, p := range m.partitions {
		m.partitionStats[p] = newPartitionMetrics(statsWindow)
	}
	return m
}

// TODO: Cross-cluster measurements should ignore partitions on e2e and not measure b2c
func NewMonitorFromConfig(cfg *config.KMonConfig, partitions []int32) (*Monitor, error) {
	producerOpts := []kgo.Opt{
		kgo.SeedBrokers(cfg.ProducerKafkaConfig.SeedBrokers...),
		kgo.RecordPartitioner(kgo.ManualPartitioner()),
	}
	if cfg.ConsumerKafkaConfig == nil {
		producerOpts = append(producerOpts, kgo.ConsumeTopics(cfg.ProducerMonitoringTopic))
	}

	// TODO: Use GetFranzGoClient
	producerClient, err := kgo.NewClient(producerOpts...)
	if err != nil {
		return nil, err
	}

	consumerClient := producerClient
	if cfg.ConsumerKafkaConfig != nil {
		consumerOpts := []kgo.Opt{
			kgo.SeedBrokers(cfg.ConsumerKafkaConfig.SeedBrokers...),
			kgo.ConsumeTopics(cfg.ConsumerMonitoringTopic),
		}

		// TODO: Use GetFranzGoClient
		consumerClient, err = kgo.NewClient(consumerOpts...)
		if err != nil {
			producerClient.Close()
			return nil, err
		}
	}

	// Generate a unique UUID for this Monitor instance
	instanceUUID := uuid.NewString()

	sampleFrequency := time.Duration(cfg.SampleFrequencyMs) * time.Millisecond
	statsWindow := time.Duration(cfg.StatsWindowSeconds) * time.Second

	return NewMonitorWithClients(producerClient, cfg.ProducerMonitoringTopic, consumerClient, instanceUUID, partitions, sampleFrequency, statsWindow), nil
}

func (m *Monitor) Start(ctx context.Context) {
	defer m.producerClient.Close()
	if m.consumerClient != m.producerClient {
		defer m.consumerClient.Close()
	}

	m.Warmup()

	go m.consumeLoop(ctx)

	ticker := time.NewTicker(m.sampleFrequency)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			go m.publishProbeBatch(ctx)
		}
	}
}

func (m *Monitor) Warmup() {

}

func (m *Monitor) publishProbeBatch(ctx context.Context) {
	for _, partition := range m.partitions {
		m.publishProbe(ctx, partition)
	}
}

func (m *Monitor) publishProbe(ctx context.Context, partition int32) {
	sentAt := time.Now()
	record := &kgo.Record{
		Topic:     m.producerTopic,
		Partition: partition,
		Key:       []byte(m.instanceUUID),
		Value:     fmt.Appendf(nil, "%d", sentAt.UnixNano()),
	}

	partitionLabel := m.partitionLabel(partition)

	m.producerClient.Produce(ctx, record, func(r *kgo.Record, err error) {
		if err != nil {
			ProduceFailureCount.WithLabelValues(partitionLabel).Inc()
			return
		}

		ackLatencyMs := float64(time.Since(sentAt).Milliseconds())
		m.getPartitionMetrics(partition).recordP2B(partitionLabel, ackLatencyMs)
	})
}

func (m *Monitor) consumeLoop(ctx context.Context) {
	for {
		fetches := m.consumerClient.PollFetches(ctx)

		select {
		case <-ctx.Done():
			return
		default:
			if fetches.IsClientClosed() {
				return
			}

			fetches.EachError(func(topic string, partition int32, err error) {
				log.Error().Err(err).Msgf("kafka fetch error on %s[%d]", topic, partition)
			})

			fetches.EachRecord(func(record *kgo.Record) {
				go m.handleConsumedRecord(record)
			})
		}
	}
}

func (m *Monitor) handleConsumedRecord(record *kgo.Record) {
	// Only process messages that were generated by this instance
	if string(record.Key) != m.instanceUUID {
		return
	}

	// If the message was produced by this instance, continue processing
	timestamp, err := strconv.ParseInt(string(record.Value), 10, 64)
	if err != nil {
		// If we can't parse the timestamp, ignore it
		return
	}
	sentAt := time.Unix(0, timestamp)

	consumeTime := time.Now()
	partition := record.Partition
	partitionLabel := m.partitionLabel(partition)

	e2eLatencyMs := float64(consumeTime.Sub(sentAt).Milliseconds())
	m.getPartitionMetrics(partition).recordE2E(partitionLabel, e2eLatencyMs)
}

func (m *Monitor) getPartitionMetrics(partition int32) *partitionMetrics {
	return m.partitionStats[partition]
}

func (m *Monitor) partitionLabel(partition int32) string {
	return fmt.Sprintf("%d", partition)
}

func newPartitionMetrics(window time.Duration) *partitionMetrics {
	return &partitionMetrics{
		p2b: utils.NewStats(window),
		e2e: utils.NewStats(window),
	}
}

func (pm *partitionMetrics) recordP2B(partitionLabel string, latencyMs float64) {
	pm.p2b.Add(int64(latencyMs))
	pm.updateQuantiles(pm.p2b, P2BMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) recordE2E(partitionLabel string, latencyMs float64) {
	pm.e2e.Add(int64(latencyMs))
	pm.updateQuantiles(pm.e2e, E2EMessageLatencyQuantile, partitionLabel)
}

func (pm *partitionMetrics) updateQuantiles(stats *utils.Stats, gauge *prometheus.GaugeVec, partitionLabel string) {
	res, ok := stats.Percentile([]float64{50, 95, 99})
	if !ok {
		return
	}
	gauge.WithLabelValues(partitionLabel, "p50").Set(float64(res[0]))
	gauge.WithLabelValues(partitionLabel, "p95").Set(float64(res[1]))
	gauge.WithLabelValues(partitionLabel, "p99").Set(float64(res[2]))
}
